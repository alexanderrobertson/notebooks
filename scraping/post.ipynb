{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting web data without an API\n",
    "\n",
    "In this tutorial, I'll show you how to extract data from Wikipedia pages.\n",
    "\n",
    "If you've ever gone through an online machine learning tutorial, you're likely to be familiar with standard datasets like [Titanic casualties](https://github.com/mwaskom/seaborn-data/blob/master/titanic.csv), [Iris flowers](https://github.com/mwaskom/seaborn-data/blob/master/iris.csv) or [customer tips](https://github.com/mwaskom/seaborn-data/blob/master/tips.csv). These simple, well-structured datasets are great for getting to grips with data science fundamentals. And once you've mastered them, you can create your own datasets to investigate anything at all. You might use public APIs to gather this data, such as the ones available for [Twitter](https://developer.twitter.com/en/docs.html), [Reddit](https://www.reddit.com/dev/api/) or [Instagram](https://www.instagram.com/developer/). Many APIs also have Python packages which make it even easier to access the data you want: [Tweepy](http://www.tweepy.org/) for Twitter or [PRAW](https://praw.readthedocs.io/en/latest/) for Reddit.\n",
    "\n",
    "If you are working with completely unstructured data in raw text form, then you could use natural language processing tools to extract key elements. [Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) techniques can identify (amongst other things) places, companies and products in text. [Amazon Comprehend](https://aws.amazon.com/comprehend/) is one cloud-based approach, or you can do it in python with [spaCy](https://spacy.io/).\n",
    "\n",
    "However, when working with web data there is often structure to it that can be exploited. So even if there isn't a nice convenient API, you can still take advantage of the fact that the data is in HTML format. And if there's not much data, it might be possible to just manually copy and paste it - a bit tedious, perhaps, but not too much trouble. For example, if you just want to extract a table or two from a webpage, you can even use [pandas.read_html()](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_html.html) to load and parse the page, automatically extracting tables as dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crash course: HTML tables\n",
    "\n",
    "Take some time to read about [HTML element tags](https://www.w3schools.com/tags/default.asp), [HTML element attributes](https://www.w3schools.com/html/html_attributes.asp) and [HTML classes](https://www.w3schools.com/html/html_classes.asp). Knowing about these parts of HTML will make it much easier for us to identify and extract the data we want.\n",
    "\n",
    "Tables in HTML have a nested structure. For a fuller introduction, look at the [w3schools.com](https://www.w3schools.com/html/html_tables.asp) guide on tables. Key points are that nested inside a `table` element are `tr` elements, which represent the rows. And nested inside the rows are `th` or `td` elements, representing the actual cells. `th` denotes the header row of the table, while `td` elements will be the standard cells. In the following code, you can see how some elements have `class` or `href` attributes.\n",
    "\n",
    "~~~~html\n",
    "<table class=\"main-table\">\n",
    "    <tr>\n",
    "        <th>Letter</th>\n",
    "        <th>Number</th>\n",
    "        <th>Country</th>\n",
    "        <th>Link</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>A</td>\n",
    "        <td>1</td>\n",
    "        <td>Germany</td>\n",
    "        <td><a href=\"www.url.com\">Germany</a>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>B</td>\n",
    "        <td>2</td>\n",
    "        <td>France</td>\n",
    "        <td><a href=\"www.url.com\">France</a>\n",
    "    </tr>\n",
    "</table>\n",
    "~~~~\n",
    "Which generates this table:\n",
    "<table class=\"main-table\">\n",
    "    <tr>\n",
    "        <th>Letter</th>\n",
    "        <th>Number</th>\n",
    "        <th>Country</th>\n",
    "        <th>Link</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>A</td>\n",
    "        <td>1</td>\n",
    "        <td>Germany</td>\n",
    "        <td><a href=\"www.url.com\">Germany</a>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>B</td>\n",
    "        <td>2</td>\n",
    "        <td>France</td>\n",
    "        <td><a href=\"www.url.com\">France</a>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "Tables can be much more complex than this, as we will see, but this nested structure is common to them all.\n",
    "\n",
    "\n",
    "\n",
    "Extracting tables from HTML is easy with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 4 table/s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>City[1]</td>\n",
       "      <td>Year grantedor confirmed</td>\n",
       "      <td>Cathedral[a]</td>\n",
       "      <td>City council</td>\n",
       "      <td>Nation/Region</td>\n",
       "      <td>Image</td>\n",
       "      <td>Population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aberdeen32(Scots: Aiberdeen)(Scottish Gaelic: ...</td>\n",
       "      <td>31(Burgh: 1179)</td>\n",
       "      <td>St Machar's, High Kirk of Aberdeen</td>\n",
       "      <td>Local government district</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Armagh11(Irish: Ard Mhacha)(Ulster-Scots: Airm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Patrick's Cathedral, Armagh (Church of Irel...</td>\n",
       "      <td>None. Represented on Armagh City, Banbridge an...</td>\n",
       "      <td>Northern Ireland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bangor1</td>\n",
       "      <td>time immemorial</td>\n",
       "      <td>Cathedral Church of St Deiniol</td>\n",
       "      <td>Community</td>\n",
       "      <td>Wales</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bath1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Abbey Church of SS Peter &amp; Paul4</td>\n",
       "      <td>Charter trustees</td>\n",
       "      <td>South West, England</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2011)[15]97,311 (urban area, 2010)[16]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0                                            City[1]   \n",
       "1  Aberdeen32(Scots: Aiberdeen)(Scottish Gaelic: ...   \n",
       "2  Armagh11(Irish: Ard Mhacha)(Ulster-Scots: Airm...   \n",
       "3                                            Bangor1   \n",
       "4                                              Bath1   \n",
       "\n",
       "                          1  \\\n",
       "0  Year grantedor confirmed   \n",
       "1           31(Burgh: 1179)   \n",
       "2                       NaN   \n",
       "3           time immemorial   \n",
       "4                       NaN   \n",
       "\n",
       "                                                   2  \\\n",
       "0                                       Cathedral[a]   \n",
       "1                 St Machar's, High Kirk of Aberdeen   \n",
       "2  St Patrick's Cathedral, Armagh (Church of Irel...   \n",
       "3                     Cathedral Church of St Deiniol   \n",
       "4                   Abbey Church of SS Peter & Paul4   \n",
       "\n",
       "                                                   3                    4  \\\n",
       "0                                       City council        Nation/Region   \n",
       "1                          Local government district             Scotland   \n",
       "2  None. Represented on Armagh City, Banbridge an...     Northern Ireland   \n",
       "3                                          Community                Wales   \n",
       "4                                   Charter trustees  South West, England   \n",
       "\n",
       "       5                                       6  \n",
       "0  Image                              Population  \n",
       "1    NaN                                      38  \n",
       "2    NaN                                    [13]  \n",
       "3    NaN                                    [14]  \n",
       "4    NaN  2011)[15]97,311 (urban area, 2010)[16]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_html('https://en.wikipedia.org/wiki/List_of_cities_in_the_United_Kingdom')\n",
    "\n",
    "print(f'Extracted {len(data)} table/s')\n",
    "\n",
    "data[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: if you aren't familiar with f-strings, which were introduced in Python 3.6 with [PEP 498](https://www.python.org/dev/peps/pep-0498/), take a look at [this blog post](https://cito.github.io/blog/f-strings/) for more details. They make it super easy to reference and format variables inside a string.)\n",
    "\n",
    "If we look at the first item extracted, it's the table showing information about all the cities in the UK. However, this would need cleaned up a bit - there are no column names, plus there is a lot of extraneous text such as footnote references and so on. This data is not very useable in its current format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling\n",
    "\n",
    "Imagine you want to not only get a list of all the cities in the UK, but extract some information about them from Wikipedia. And you don't want to click on anything! We'll use [requests](http://docs.python-requests.org/en/master/) and [beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to do this. Wikipedia articles like these contain a vast amount of information, but we will focus on the following:\n",
    "\n",
    "* Which country it is in\n",
    "* The current population\n",
    "* A list of other Wikipedia pages the city article links to in its introduction\n",
    "\n",
    "The first three items on this list will require extracting data from tables. The last item will extract data that is not in a table, which pandas would not be able to extract.\n",
    "\n",
    "The process looks like this: \n",
    "\n",
    "1. Generate a list of Wikipedia URLs for the cities of the UK\n",
    "2. Access each URL and extract the above information\n",
    "3. Save the extracted information to disk for analysis later on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the URLs from a Wikipedia page\n",
    "\n",
    "Although we can use pandas to get the text of the table of cities, it doesn't get the URLs. Another option would be to just copy and paste the links from the page, but that almost defeats the point of knowing Python. We can automate this process.\n",
    "\n",
    "There are some packages that can do this for us if we have some knowledge about how HTML is structured:\n",
    "\n",
    "* [requests](http://docs.python-requests.org/en/master/) to get the HTML. This is a great package for accessing URLs and is incredibly simple to use\n",
    "\n",
    "* [beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to parse that HTML and extract the items we want. Also very simple to use, but it helps to know a bit about HTML beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Status code: 200\n",
      "      Reason for code: OK\n",
      " HTML chars retrieved: 301963\n",
      "\n",
      "HTML sample:\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>List of cities in the United Kingdom - Wikipedia</title>\n",
      "<script>document.documentElement.className = document.documentElement.className.replace( /(^\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "city_list = requests.get('https://en.wikipedia.org/wiki/List_of_cities_in_the_United_Kingdom')\n",
    "\n",
    "print(f'{\"Status code:\":>22} {city_list.status_code}')\n",
    "print(f'{\"Reason for code:\":>22} {city_list.reason}')\n",
    "print(f'{\"HTML chars retrieved:\":>22} {len(city_list.text)}')\n",
    "\n",
    "print('\\nHTML sample:\\n')\n",
    "print(city_list.text[0:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `city_list` variable contains all the HTTP data for the URL we accessed. The request was successful, as a [status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) of 200 was returned, and around 300k characters of HTML were retrieved.\n",
    "\n",
    "Before information can be extracted from that HTML, it must be parsed. This lets us refer to elements in the data according to various properties (such as class or attribute) and is much easier than using regular expressions.\n",
    "\n",
    "At this point, we need to find the \"List of cities\" table in the HTML. This can be done by viewing the page source (available from the right-click menu in most web browsers) and searching for `<table` to find the start of any tables. This page contains four but the one we want looks like this:\n",
    "\n",
    "<img src=\"notebook_images/search_source.png\" />\n",
    "\n",
    "This table is of the class `wikitable sortable`. We can use [beautifulsoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to parse the HTML and search for all `table` elements with a class of `wikitable sortable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 table/tables.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "city_list = requests.get('https://en.wikipedia.org/wiki/List_of_cities_in_the_United_Kingdom')\n",
    "city_list = bs(city_list.text, \"lxml\")\n",
    "\n",
    "table = city_list.find_all('table', {'class':'wikitable sortable'})\n",
    "\n",
    "print(f'Found {len(table)} table/tables.')\n",
    "\n",
    "table = table[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, there is only one table with that class, so there is no need to examine all the results and find the correct item. We can just use the single item in the list that the `find_all()` function returns.\n",
    "\n",
    "HTML tables are organised by rows, which are `tr` elements inside the parent `table`. These can be extracted using `find_all()` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70 row/rows.\n"
     ]
    }
   ],
   "source": [
    "table_rows = table.find_all('tr')\n",
    "\n",
    "print(f'Found {len(table_rows)} row/rows.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 69 cities in the UK, so it looks like we've got the data we need plus an extra header row.\n",
    "\n",
    "The cells in each row are `td` elements. The exception is the header row, which are `th`. Again, these can be extracted from each row using `find_all()`. We only want the first column of the table, which has the links to the cities' pages. This link is an `a` element, which can be extracted just like the other elements. It's clear from the HTML source that the first link is what we want. Once this has been found, it is the `href` attribute that needs to be extracted - this is the actual link. Finally, the links will be relative rather than absolute, so it will be necessary to prepend the domain name onto them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 69 links. Is this the right number? Yes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://en.wikipedia.org/wiki/Aberdeen',\n",
       " 'https://en.wikipedia.org/wiki/Armagh',\n",
       " 'https://en.wikipedia.org/wiki/Bangor,_Gwynedd',\n",
       " 'https://en.wikipedia.org/wiki/Bath,_Somerset']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = []\n",
    "\n",
    "for row in table_rows:\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    if cells:\n",
    "        link = cells[0].find_all('a')[0]\n",
    "        link = link.attrs['href']\n",
    "        links.append(f'https://en.wikipedia.org{link}')\n",
    "    else:\n",
    "        # We found \"th\" header cells in this row (or\n",
    "        # something else we don't want), instead of\n",
    "        # \"td\", so pass silently and do nothing\n",
    "        pass\n",
    "\n",
    "print(f'Extracted {len(links)} links. Is this the right number? {\"Yes\" if len(links) == 69 else \"No\"}.')\n",
    "\n",
    "links[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our links, we can loop through them and extract required information from each page. In the following part, I'll use the example of [Leeds](https://en.wikipedia.org/wiki/City_of_Leeds) to show you how to use the structure of HTML to extract what we need. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which country is the city in?\n",
    "\n",
    "This information can be found in the info box to the right of the Wikipedia page, next to the cell named \"Constituent country\".\n",
    "\n",
    "<img src=\"notebook_images/country.png\" />\n",
    "\n",
    "Inspecting the HTML source shows that this is inside a `tr` of class `mergedrow`. There is a header `th` cell with the text \"Constituent country\" and the data we want is in the text of the adjacent `td` cell.\n",
    "\n",
    "<img src=\"notebook_images/mergedrow.png\" />\n",
    "\n",
    "(Note: When trying to examine the structure of the HTML, you will probably find it useful to view it in an editor that does syntax highlighting and will prettify the code to show the nested structure. Many text editors, such as [SublimeText](https://www.sublimetext.com/3), have plugins that will do this if they do not do it natively.)\n",
    "\n",
    "To extract this, we can search for all the `mergedrow` elements. Then we loop through all the pairs until we find what we want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country is England\n"
     ]
    }
   ],
   "source": [
    "city = requests.get('https://en.wikipedia.org/wiki/City_of_Leeds')\n",
    "city = bs(city.text, 'lxml')\n",
    "\n",
    "rows = city.find_all('tr', {'class':'mergedrow'})\n",
    "\n",
    "for header, cell in rows:\n",
    "    if 'country' in header.text:\n",
    "        print(f'Country is {cell.text.strip()}')\n",
    "        # Use .strip() because the text has linebreaks in it that we want to remove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "This works for Leeds, but what about the other cities? Unfortunately, the Wikipedia pages are not entirely consistent in how they structure their HTML. It will be necessary to check the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not get data for Newport,_Wales!\n",
      "Got data for 68 of 69 cities.\n"
     ]
    }
   ],
   "source": [
    "country_data = {}\n",
    "\n",
    "for c in links:\n",
    "    \n",
    "    name = c.split('/')[-1]\n",
    "    # Get the city name from the URL\n",
    "    \n",
    "    city = requests.get(c)\n",
    "    city = bs(city.text, 'lxml')\n",
    "\n",
    "    rows = city.find_all('tr', {'class':'mergedrow'})\n",
    "\n",
    "    for r in rows:\n",
    "        if len(r) == 2:\n",
    "            header, cell = r\n",
    "            if 'country' in header.text.lower():\n",
    "                country_data[name] = cell.text.strip()\n",
    "                # No need to keep looking once a match is found\n",
    "                break\n",
    "                \n",
    "    if not country_data.get(name):\n",
    "        print(f'Did not get data for {name}!')\n",
    "    \n",
    "print(f'Got data for {len(country_data)} of {len(links)} cities.')\n",
    "# If a country was found for the city, there will be a dictionary entry for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So one city is missing the data we need. That's easily checked and it seems the problem is that the Wikipedia entry for the city of Newport in Wales uses the word \"part\" instead of \"country\"\n",
    "\n",
    "(Note: this is a very common problem when scraping data from HTML! You think you've figured it all out and then the edge cases start popping up, where some tiny difference makes it impossible to extract what you want.)\n",
    "\n",
    "This is easily fixed by checking for \"part\" and \"country\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got data for 69 of 69 cities.\n"
     ]
    }
   ],
   "source": [
    "country_data = {}\n",
    "\n",
    "for c in links:\n",
    "    \n",
    "    name = c.split('/')[-1]\n",
    "    # Get the city name from the URL\n",
    "    \n",
    "    city = requests.get(c)\n",
    "    city = bs(city.text, 'lxml')\n",
    "\n",
    "    rows = city.find_all('tr', {'class':'mergedrow'})\n",
    "\n",
    "    for r in rows:\n",
    "        if len(r) == 2:\n",
    "            header, cell = r\n",
    "            if 'country' in header.text.lower() or 'part' in header.text.lower():\n",
    "                country_data[name] = cell.text.strip()\n",
    "                # No need to keep looking once a match is found\n",
    "                break\n",
    "                \n",
    "    if country_data.get(name):\n",
    "        # Evaluates as True only if key in dictionary\n",
    "        pass\n",
    "    else:\n",
    "        print(f'Did not get data for {name}!')\n",
    "    \n",
    "print(f'Got data for {len(country_data)} of {len(links)} cities.')\n",
    "# If a country was found for the city, there will be a dictionary entry for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the current population?\n",
    "\n",
    "The HTML source shows that this is going to be trickier: the population data is not neatly marked with a convenient header in the table. From looking at the entry for Leeds:\n",
    "\n",
    "<img src=\"notebook_images/population.png\" />\n",
    "\n",
    "The data we want is inside a `td` element but this has no useful attributes we can use to identify it. However, the preceding `tr` element, with class `mergedtoprow`, contains the word \"Population\". So we can target that and get its next sibling in the HTML structure. This is the `tr` with `mergedrow`: then we just need to get its children and select the `td` element, which we know (from the HTML) is the second child in the returned iterator. The text can be extracted from this and cleaned up.\n",
    "\n",
    "This time, let's check all the cities at once and clean up as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aberdeen 3,505.7/km2\n",
      "Armagh None\n",
      "Bangor,_Gwynedd None\n",
      "Bath,_Somerset None\n",
      "Belfast None\n",
      "Birmingham 1,137,100\n",
      "City_of_Bradford 534,800\n",
      "Brighton_and_Hove 288,200\n",
      "Bristol 459,300\n",
      "Cambridge 124,900\n",
      "City_of_Canterbury 164,100\n",
      "Cardiff 361,468[1]\n",
      "City_of_Carlisle 108,300\n",
      "City_of_Chelmsford 176,200\n",
      "Chester None\n",
      "Chichester None\n",
      "Coventry 360,100\n",
      "Derby 248,700\n",
      "Derry None\n",
      "Dundee 148,270\n",
      "Durham,_England 48,069\n",
      "Edinburgh 464,990\n",
      "Ely,_Cambridgeshire None\n",
      "Exeter 128,900\n",
      "Glasgow 621,020[1]\n",
      "Gloucester 129,100\n",
      "Hereford None\n",
      "Inverness None\n",
      "Kingston_upon_Hull 260,700\n",
      "City_of_Lancaster 142,500\n",
      "City_of_Leeds 784,800\n",
      "Leicester 329,839[1]\n",
      "Lichfield None\n",
      "Lincoln,_England 97,541[1]\n",
      "Lisburn None\n",
      "Liverpool 491,500\n",
      "City_of_London 9,401\n",
      "Manchester 545,500\n",
      "Newcastle_upon_Tyne 295,800\n",
      "Newport,_Wales 151,500\n",
      "Newry None\n",
      "Norwich 141,300\n",
      "Nottingham 321,500\n",
      "Oxford 151,906\n",
      "Perth,_Scotland None\n",
      "City_of_Peterborough 198,900\n",
      "Plymouth 263,100\n",
      "Portsmouth 205,100\n",
      "City_of_Preston,_Lancashire 141,300\n",
      "Ripon None\n",
      "City_and_District_of_St_Albans (of\n",
      "St_Asaph None\n",
      "St_Davids None\n",
      "City_of_Salford 251,300\n",
      "Salisbury None\n",
      "City_of_Sheffield 577,800\n",
      "Southampton 236,900\n",
      "Stirling None\n",
      "Stoke-on-Trent 255,400\n",
      "City_of_Sunderland 277,962\n",
      "Swansea Unitary\n",
      "Truro None\n",
      "City_of_Wakefield 340,800\n",
      "Wells,_Somerset None\n",
      "City_of_Westminster 244,800\n",
      "City_of_Winchester 123,900\n",
      "Wolverhampton 259,900\n",
      "Worcester 101,328\n",
      "City_of_York 208,200\n"
     ]
    }
   ],
   "source": [
    "population_data = {}\n",
    "\n",
    "for c in links:\n",
    "    \n",
    "    name = c.split('/')[-1]\n",
    "    # Get the city name from the URL\n",
    "    \n",
    "    city = requests.get(c)\n",
    "    city = bs(city.text, 'lxml')\n",
    "\n",
    "    rows = city.find_all('tr', {'class':'mergedtoprow'})\n",
    "\n",
    "    population = None\n",
    "    \n",
    "    for r in rows:\n",
    "        if 'Population' in r.text:\n",
    "            sibling = r.next_sibling\n",
    "            td_child = list(sibling.children)[1]\n",
    "\n",
    "            population = td_child.text.strip()\n",
    "            # Remove linebreaks from start/end\n",
    "            population = population.split()[0]\n",
    "            # Get the first item before a space\n",
    "\n",
    "            population_data[name] = population\n",
    "            break\n",
    "            \n",
    "    print(name, population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear! It looks like we've grabbed the wrong HTML element for some cities and instead got the area, or failed to get anything at all.\n",
    "\n",
    "To fix this, we need to dig around in the HTML for cities that failed and edit our code. Here's what the result of that should look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_data = {}\n",
    "\n",
    "for c in links:\n",
    "    \n",
    "    name = c.split('/')[-1]\n",
    "    # Get the city name from the URL\n",
    "    \n",
    "    city = requests.get(c)\n",
    "    city = bs(city.text, 'lxml')\n",
    "        \n",
    "    population = None\n",
    "                   \n",
    "    if name in ['Derry']:\n",
    "        rows = city.find_all('tr')\n",
    "        for r in rows:\n",
    "            if 'Population' in r.text:\n",
    "                population = r.find_all('li')[0].text\n",
    "                break\n",
    "    \n",
    "    elif name in ['Aberdeen', 'Armagh', 'Bangor,_Gwynedd', 'Bath,_Somerset', 'Belfast', 'Chester',\n",
    "                  'Chichester', 'Hereford', 'Inverness', 'Ely,_Cambridgeshire', 'Lichfield',\n",
    "                  'Lisburn', 'Newry', 'Perth,_Scotland', 'Ripon', 'Stirling', 'Truro',\n",
    "                  'Wells,_Somerset']:\n",
    "        rows = city.find_all('tr')\n",
    "        for r in rows:\n",
    "            if 'Population' in r.text:\n",
    "                children = list(r.children)\n",
    "                population = children[1].text.replace('\\n', ' ').strip()\n",
    "                break  \n",
    "    else:\n",
    "        rows = city.find_all('tr', {'class':'mergedtoprow'})\n",
    "        for r in rows:\n",
    "            if 'Population ' in r.text:\n",
    "                population = r.next_sibling.text.replace('\\n', ' ').strip()\n",
    "                break\n",
    "                \n",
    "    population_data[name] = population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aberdeen                       : 196,670[1] – City proper\n",
      "Armagh                         : 14,749 (2011 Census)\n",
      "Bangor,_Gwynedd                : 18,810 (2011 census)\n",
      "Bath,_Somerset                 : 88,859 [1]\n",
      "Belfast                        : City of Belfast:340,200 (2017)[2] Urban Area:483,418 (2016)[3]Metropolitan area:671,559 (2011)[4]\n",
      "Birmingham                     : • City 1,137,100\n",
      "City_of_Bradford               : • Total 534,800 (Ranked 6th)\n",
      "Brighton_and_Hove              : • City and unitary authority 288,200\n",
      "Bristol                        : • City and county 459,300 (Ranked 10th district and 43rd ceremonial county)\n",
      "Cambridge                      : • City and non-metropolitan district 124,900 (ranked 183rd)\n",
      "City_of_Canterbury             : • Total 164,100\n",
      "Cardiff                        : • City & County 361,468[1]\n",
      "City_of_Carlisle               : • Total 108,300 (Ranked 221st)\n",
      "City_of_Chelmsford             : • Total 176,200\n",
      "Chester                        : 118,200 [1]\n",
      "Chichester                     : 26,795 [2] 2011 Census\n",
      "Coventry                       : • City and Metropolitan borough 360,100 (Ranked 15th)\n",
      "Derby                          : • City and Unitary authority area 248,700\n",
      "Derry                          : Derry: 85,016\n",
      "Dundee                         : • Total 148,270\n",
      "Durham,_England                : • Total 48,069 (urban area)[1]\n",
      "Edinburgh                      : • City and council area 464,990 – Locality [1] 513,210 – Local Authority Area[2]\n",
      "Ely,_Cambridgeshire            : 20,112\n",
      "Exeter                         : • Total 128,900\n",
      "Glasgow                        : • City 621,020[1]\n",
      "Gloucester                     : • Total 129,100\n",
      "Hereford                       : 58,896 [1]\n",
      "Inverness                      : 63,780 [1]\n",
      "Kingston_upon_Hull             : • City 260,700 (Ranked 58th)\n",
      "City_of_Lancaster              : • Total 142,500\n",
      "City_of_Leeds                  : • Total 784,800 (Ranked 2nd)\n",
      "Leicester                      : • City 329,839[1]\n",
      "Lichfield                      : 32,219 (2011)[2]\n",
      "Lincoln,_England               : • City and Borough 97,541[1]\n",
      "Lisburn                        : 120,465 surrounding areas\n",
      "Liverpool                      : • City 491,500\n",
      "City_of_London                 : • City 9,401\n",
      "Manchester                     : • City 545,500\n",
      "Newcastle_upon_Tyne            : • City 295,800 (ranked 40th district)\n",
      "Newport,_Wales                 : • Total 151,500 (Ranked 7th)\n",
      "Newry                          : 26,967 (2011)[4]\n",
      "Norwich                        : • City 141,300 (ranked 146th)\n",
      "Nottingham                     : • City 321,500\n",
      "Oxford                         : • City and non-metropolitan district 151,906 (2,011)[1]\n",
      "Perth,_Scotland                : 47,180 (est. 2012), excluding suburbs[3]\n",
      "City_of_Peterborough           : • Total 198,900\n",
      "Plymouth                       : • Total 263,100\n",
      "Portsmouth                     : • City & unitary authority area 205,100 (Ranked  76th)[2]\n",
      "City_of_Preston,_Lancashire    : • City & Non-metropolitan district 141,300 (Ranked 146th)\n",
      "Ripon                          : 16,702 (2011 census)[1]\n",
      "City_and_District_of_St_Albans : • Rank (of 326)\n",
      "!! Nothing found for St_Asaph\n",
      "!! Nothing found for St_Davids\n",
      "City_of_Salford                : • Total 251,300 (Ranked 65th)\n",
      "!! Nothing found for Salisbury\n",
      "City_of_Sheffield              : • City 577,800 (Ranked 3rd)\n",
      "Southampton                    : • City and unitary authority area 236,900 (Ranked 57)\n",
      "Stirling                       : 36,142 Census 2011 [1]\n",
      "Stoke-on-Trent                 : • City 255,400\n",
      "City_of_Sunderland             : • Total 277,962\n",
      "Swansea                        : • Total  Unitary Authority area: 245,500, Ranked 2nd Urban area within Unitary Authority: 179,485 Wider Urban Area: 300,352 Metropolitan Area: 462,000 Swansea Bay City Region: 685,051\n",
      "Truro                          : 18,766 [1]\n",
      "City_of_Wakefield              : • Total 340,800\n",
      "Wells,_Somerset                : 10,536 (2011)[1]\n",
      "City_of_Westminster            : • Total 244,800\n",
      "City_of_Winchester             : • Total 123,900\n",
      "Wolverhampton                  : • Total 259,900 (59th)\n",
      "Worcester                      : • Total 101,328\n",
      "City_of_York                   : • Total 208,200 (Ranked 84th)\n"
     ]
    }
   ],
   "source": [
    "for name, population in population_data.items():\n",
    "    if population:\n",
    "        print(f'{name:<30} : {population:}')\n",
    "    else:\n",
    "        print(f'!! Nothing found for {name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the HTML fell into three distinct categories. Derryhad a list structure inside a table cell. Several cities had rows without the `mergedtoprow` class name. But most used the sme structure as Leeds.\n",
    "\n",
    "Some cities did not actually have population data listed inside tables (e.g. St.Asaph, St.David). This will likely have to be added manually.\n",
    "\n",
    "A final issue is that the data extracted is still very messy. Ideally it would be nicely formatted as integers. However, a few [regular expressions](https://docs.python.org/3/library/re.html) will fix that all up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting links\n",
    "\n",
    "The final bit of information we'll extract is all the links contained in the introduction for each city. In general, the introduction is the text that appears before the table of contents. Each paragraph is wrapped in a `p` element in HTML and the table of contents has an `id` of `toc`. Because `id` [must be unique](https://www.w3schools.com/html/html_id.asp) in an HTML document, it will be easy to identify the TOC. The Wiki pages we are working with here have the general structure:\n",
    "\n",
    "<img src=\"notebook_images/toc.png\" />\n",
    "\n",
    "First comes the infobox table, which we've already extracted data from. Then a series of paragraphs of text. Then a `div` containing the table of contents. All we need to do is loop through the `p` elements and keep checking to see if that element has a sibling below it on the page which has a particular `id` and, if so, stop looping through the paragraphs. And for each paragraph, we'll extract the `a` elements to get the links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_data = {}\n",
    "\n",
    "for c in links:\n",
    "    \n",
    "    name = c.split('/')[-1]\n",
    "    # Get the city name from the URL\n",
    "    \n",
    "    city = requests.get(c)\n",
    "    city = bs(city.text, 'lxml')\n",
    "        \n",
    "    intro_links = []\n",
    "    introduction_paragraphs = []\n",
    "\n",
    "    all_paragraphs = city.find_all('p')\n",
    "\n",
    "    for p in all_paragraphs:\n",
    "        # Find all the links inside a paragraph\n",
    "        found_links = set([l.attrs['href'] for l in p.find_all('a')])\n",
    "\n",
    "        intro_links.extend([i for i in found_links if i.startswith('/wiki/') and ':' not in i])\n",
    "        # Only want links that go to Wikipedia pages, but exclude Help: pages \n",
    "\n",
    "        next_sibling = p.find_next_sibling()\n",
    "        # Check the next item in the HTML structure that comes after this paragraph\n",
    "\n",
    "        if next_sibling:\n",
    "            # If it exists...\n",
    "            if next_sibling.get('id') == 'toc':\n",
    "                # And if it has the toc id\n",
    "                break\n",
    "                    # Then have eached TOC, so stop processing further paragraphs.\n",
    "    \n",
    "    intro_data[name] = intro_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 23 links for Aberdeen\n",
      " 23 links for Armagh\n",
      " 12 links for Bangor,_Gwynedd\n",
      " 41 links for Bath,_Somerset\n",
      " 20 links for Belfast\n",
      " 47 links for Birmingham\n",
      " 53 links for City_of_Bradford\n",
      "  8 links for Brighton_and_Hove\n",
      " 58 links for Bristol\n",
      " 32 links for Cambridge\n",
      "['/wiki/List_of_towns_and_cities_in_Scotland_by_population', '/wiki/City_status_in_the_United_Kingdom', '/wiki/List_of_urban_areas_in_the_United_Kingdom', '/wiki/Local_government_in_Scotland', '/wiki/Scots_language']\n"
     ]
    }
   ],
   "source": [
    "for city, link_list in list(intro_data.items())[0:10]:\n",
    "    print(f'{len(link_list):>3} links for {city}')\n",
    "    \n",
    "print(intro_data['Aberdeen'][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting it all together\n",
    "\n",
    "With the code written for extracting the three required kinds of information, we can put together a little script that will do everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "links = [] # Store links to city Wiki pages\n",
    "\n",
    "city_list = requests.get('https://en.wikipedia.org/wiki/List_of_cities_in_the_United_Kingdom')\n",
    "city_list = bs(city_list.text, \"lxml\")\n",
    "table = city_list.find_all('table', {'class':'wikitable sortable'})\n",
    "table = table[0]\n",
    "# Download and parse HTML list of UK cities\n",
    "\n",
    "for row in table_rows:\n",
    "    cells = row.find_all('td')\n",
    "    \n",
    "    if cells:\n",
    "        link = cells[0].find_all('a')[0]\n",
    "        link = link.attrs['href']\n",
    "        links.append(f'https://en.wikipedia.org{link}')\n",
    "    else:\n",
    "        # We found \"th\" header cells, instead of \"td\"\n",
    "        pass\n",
    "    \n",
    "country_data = {}    # Store info about which country each city is in\n",
    "population_data = {} # Store info about the population of each city\n",
    "intro_data = {}      # Store info about links in city introduction text\n",
    "    \n",
    "for c in links:\n",
    "    name = c.split('/')[-1]\n",
    "    # Get the city name from the URL\n",
    "    \n",
    "    city = requests.get(c)\n",
    "    city = bs(city.text, 'lxml')\n",
    "    # Download and parse the HTML\n",
    "\n",
    "    rows = city.find_all('tr', {'class':'mergedrow'})\n",
    "    # Access the rows of the table we want\n",
    "\n",
    "    for r in rows:\n",
    "        if len(r) == 2:\n",
    "            header, cell = r\n",
    "            if 'country' in header.text.lower() or 'part' in header.text.lower():\n",
    "                country_data[name] = cell.text.strip()\n",
    "                # No need to keep looking once a match is found\n",
    "                break\n",
    "        \n",
    "    population = None\n",
    "                   \n",
    "    if name in ['Derry']:\n",
    "        rows = city.find_all('tr')\n",
    "        for r in rows:\n",
    "            if 'Population' in r.text:\n",
    "                population = r.find_all('li')[0].text\n",
    "                break\n",
    "    \n",
    "    elif name in ['Aberdeen', 'Armagh', 'Bangor,_Gwynedd', 'Bath,_Somerset', 'Belfast', 'Chester',\n",
    "                  'Chichester', 'Hereford', 'Inverness', 'Ely,_Cambridgeshire', 'Lichfield',\n",
    "                  'Lisburn', 'Newry', 'Perth,_Scotland', 'Ripon', 'Stirling', 'Truro',\n",
    "                  'Wells,_Somerset']:\n",
    "        rows = city.find_all('tr')\n",
    "        for r in rows:\n",
    "            if 'Population' in r.text:\n",
    "                children = list(r.children)\n",
    "                population = children[1].text.replace('\\n', ' ').strip()\n",
    "                break  \n",
    "    else:\n",
    "        rows = city.find_all('tr', {'class':'mergedtoprow'})\n",
    "        for r in rows:\n",
    "            if 'Population ' in r.text:\n",
    "                population = r.next_sibling.text.replace('\\n', ' ').strip()\n",
    "                break\n",
    "                \n",
    "    population_data[name] = population\n",
    "\n",
    "    intro_links = []\n",
    "    introduction_paragraphs = []\n",
    "\n",
    "    all_paragraphs = city.find_all('p')\n",
    "\n",
    "    for p in all_paragraphs:\n",
    "        # Find all the links inside a paragraph\n",
    "        found_links = set([l.attrs['href'] for l in p.find_all('a')])\n",
    "\n",
    "        intro_links.extend([i for i in found_links if i.startswith('/wiki/') and ':' not in i])\n",
    "        # Only want links that go to Wikipedia pages, but exclude Help: pages \n",
    "\n",
    "        next_sibling = p.find_next_sibling()\n",
    "        # Check the next item in the HTML structure that comes after this paragraph\n",
    "\n",
    "        if next_sibling:\n",
    "            # If it exists...\n",
    "            if next_sibling.get('id') == 'toc':\n",
    "                # And if it has the toc id\n",
    "                break\n",
    "                    # Then have eached TOC, so stop processing further paragraphs.\n",
    "    \n",
    "    intro_data[name] = intro_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collating the data\n",
    "\n",
    "The extracted data is in three dictionaries, `country_data`, `population_data`, `intro_data`. There are a variety of ways to structure this for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Aberdeen</th>\n",
       "      <td>Scotland</td>\n",
       "      <td>196,670[1] – City proper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Armagh</th>\n",
       "      <td>Northern Ireland</td>\n",
       "      <td>14,749 (2011 Census)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bangor,_Gwynedd</th>\n",
       "      <td>Wales</td>\n",
       "      <td>18,810 (2011 census)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bath,_Somerset</th>\n",
       "      <td>England</td>\n",
       "      <td>88,859 [1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Belfast</th>\n",
       "      <td>Northern Ireland</td>\n",
       "      <td>City of Belfast:340,200 (2017)[2] Urban Area:4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          country  \\\n",
       "Aberdeen                 Scotland   \n",
       "Armagh           Northern Ireland   \n",
       "Bangor,_Gwynedd             Wales   \n",
       "Bath,_Somerset            England   \n",
       "Belfast          Northern Ireland   \n",
       "\n",
       "                                                        population  \n",
       "Aberdeen                                  196,670[1] – City proper  \n",
       "Armagh                                        14,749 (2011 Census)  \n",
       "Bangor,_Gwynedd                               18,810 (2011 census)  \n",
       "Bath,_Somerset                                          88,859 [1]  \n",
       "Belfast          City of Belfast:340,200 (2017)[2] Urban Area:4...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_data_df = pd.DataFrame.from_dict(country_data, orient='index', columns=['country'])\n",
    "population_data_df = pd.DataFrame.from_dict(population_data, orient='index', columns=['population'])\n",
    "\n",
    "combined_data = pd.concat([country_data_df, population_data_df], axis=1)\n",
    "\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the link data, it might be interesting to visualise it as a [graph](https://blog.cambridgespark.com/an-introduction-to-graphs-f670d29cc953). That will make it possible to see how cities are connected in terms of the concepts and ideas used to define them in Wikipedia.\n",
    "\n",
    "First, we'll get the data into a suitable format for graphing, then save it for use in [Gephi](https://gephi.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data = {'nodes':[], 'links':[]}\n",
    "\n",
    "for city in intro_data:\n",
    "    graph_data['nodes'].append({'name':city, 'group':country_data_df.loc[city].country, 'n_label':city})\n",
    "    # Create a node for the city. Get the country it is from our other data, for use in visualisation.\n",
    "    # And add a label to it for visualisation too.\n",
    "    for link in intro_data[city]:\n",
    "        graph_data['links'].append({'source':city, 'target':link})\n",
    "        # Also create a node for each wiki page that is not a city\n",
    "        # But no labels for it since we won't visualise those - too messy        \n",
    "        \n",
    "        link_info = {'name':link, 'group':'Wiki Page', 'n_label':' '}\n",
    "        # Add a link between the city and each wiki page it references.\n",
    "        if link_info not in graph_data['nodes']:\n",
    "            # Need to avoid duplicates\n",
    "            graph_data['nodes'].append(link_info)\n",
    "            \n",
    "import igraph\n",
    "\n",
    "g = igraph.Graph.DictList(vertices=graph_data['nodes'], edges=graph_data['links'], directed=True)\n",
    "g.write_graphml('wikicities.graphml')\n",
    "# Save the data in structure-preserving format for Gephi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data can then be loaded into Gephi and visualised. I highly recommend playing around with it! It makes is very easy to arrange the layout of the graph as well as apply colours and labels.\n",
    "\n",
    "Below, you can see that I've coloured the city nodes/labels according to the country they are in. Edges are coloured according to the colour of their source node. It's clear that cities in the same country share a lot of links in common.\n",
    "\n",
    "<img src=\"notebook_images/graph.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "* Try cleaning up the population data and using it in Gephi to set the size of the nodes for cities, so that larger cities appear bigger in the graph.\n",
    "* If you want to avoid manual cleaning each data point, read up on, part of the [standard library in Python](https://docs.python.org/3/library/re.html). There are some great websites for testing your regex skills: [regexr](https://regexr.com/), [pyregex](http://www.pyregex.com/).\n",
    "* Want to practice working with HTML in beautifulsoup? Try extracting data from [toscrape.com/](http://toscrape.com/), which has lots of different types of webpage to work with.\n",
    "* If you want to extend the code above to not only follow the links on the [List of UK cities](https://en.wikipedia.org/wiki/List_of_cities_in_the_United_Kingdom) page, but also follow links from *those* links, take a look at the [scrapy](https://scrapy.org/) framework, which makes it easy to recursively crawl a website."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
